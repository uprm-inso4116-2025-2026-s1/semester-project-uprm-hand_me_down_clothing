=== *3.2 Validation and Verification*

Validation and verification activities ensure that the Hand Me Down project’s 
deliverables meet the intended quality standards, align with stakeholder needs, 
and remain consistent with the domain description, requirements, and architecture. 
These activities are conducted as an ongoing, shared responsibility across all teams.

==== Validation

Validation activities focus on confirming that our understanding of the domain and 
requirements is accurate and complete, even before implementation:

* *Scenario Walkthroughs*:: Draft scenarios were created and reviewed internally 
  within the team to test whether our domain terminology, requirements, and workflows 
  are coherent. For example, walkthroughs of donation and resale flows helped 
  highlight whether our categorization and pricing assumptions were consistent.  
* *Requirement Exploration*:: At this stage, no external validation with students 
  or families has been performed. Instead, internal reviews are used to identify 
  potential gaps, such as safety practices for meetups or the way condition 
  disclosure norms are represented.

==== Verification

Verification ensures that system components behave as prescribed and that 
deliverables can be tested against measurable criteria:

* *Unit Testing*:: Individual components of the system will be tested using unit 
  tests to verify correctness in isolation. This includes authentication functions, 
  item listing logic, and condition rating updates.
* *Integration Testing*:: Selected flows (e.g., creating a listing, completing a 
  transaction) will be tested across multiple modules to ensure interactions 
  remain consistent.
* *Load Testing with k6*:: Performance and scalability will be evaluated using the 
  k6 framework. Initial load tests will target ~150 concurrent users, scaling up 
  to 500, to verify that response times remain within the defined machine 
  requirements.
* *Traceability Checks*:: Deliverables will be reviewed against the requirements 
  and terminology to ensure completeness and consistency. Each requirement will 
  be linked to validation and verification artifacts (test cases, scenarios).
* *Planned A/B Testing*:: As the UI/UX evolves, A/B testing will be used to validate 
  design hypotheses. Examples include testing filter placement in the item search 
  page, or checkout button positioning. Metrics such as time-to-action and user 
  satisfaction will guide design decisions.

==== Roles and Responsibilities

Validation and verification are a *shared responsibility* across the team.  
While individual sub-teams focus on different aspects (e.g., listings, 
authentication, documentation), every member contributes to writing, reviewing, 
and executing tests and walkthroughs. This ensures that quality is embedded 
throughout the project rather than isolated in a single role.

==== Outcomes

The combination of walkthroughs, scenario-based validation, unit and integration 
testing, load testing, and A/B experimentation provides a balanced approach:  

* *Validation* ensures that the concepts and requirements are correctly understood 
  and aligned with the domain.  
* *Verification* ensures that implementation artifacts conform to measurable 
  standards and deliver the expected performance.  

Together, these practices guarantee that the Hand Me Down project remains 
trustworthy, usable, and scalable in the context of secondhand exchanges.


=== 3.2 Validation and Verification

Validation and verification ensure that the Hand Me Down platform meets stakeholder needs,
maintains internal consistency across modules, and satisfies measurable quality standards
defined in the requirements.  These activities are carried out continuously and collaboratively
across all sub-teams—Documentation & Requirements, Authentication, Listings, Map/Search, and UI/UX—
to preserve traceability from stakeholder needs through implementation and testing.

==== Validation

*Domain and Requirements Validation*::  
Validation of terminology and requirements was performed iteratively through internal
documentation reviews.  Each update to §§ 2.1 – 2.3 was examined to confirm that
domain concepts such as *Piece*, *Listing*, *Condition Disclosure Norm*, and *Listing Closed*
remained coherent and aligned with stakeholder needs (§ 1.2.2).  
All requirements now employ definitive “shall/must” statements instead of uncertain
language (“may,” “aims”) to ensure they are directly testable.  
No external stakeholder validation sessions have yet occurred; these will take place in later
milestones to confirm usability and trust cues with students and families.

*Scenario Walkthroughs*::  
Walkthroughs were conducted for the *publishing* process, tracing data flow from
form submission to Supabase storage.  No inconsistencies were found.
Editing, closing, and saving workflows are planned for final-phase validation once their
implementations are complete.

*Category and Condition Refinement*::  
Internal validation led to refinement of category taxonomy and condition-rating scales,
resolving ambiguities from the initial model.  Terminology was standardized to
**Donated Piece** and **Sold Piece** to ensure semantic consistency across documentation and database layers.

*Search and Map Validation*::  
The Map & Search module was validated by cross-checking UI results against the actual
data stored in Supabase.  
Each search query correctly displayed only listings matching its attributes, and filter
walkthroughs confirmed accurate behavior for “Tops,” “Bottoms,” and “All.”  
Usability was validated through manual inspection: map markers are accurately pinned,
display complete popup information (name, address, hours), and maintain expected
cluster and static behaviors.  
Search relevance and marker accuracy jointly confirm correct data binding between UI
and repository functions.

*Planned Stakeholder Validation*::  
A short validation session with student volunteers will be scheduled before the final milestone
to evaluate clarity of interface terminology, filter usability, and trust indicators such as
condition labels and safety guidance.

==== Verification

*Traceability and Acceptance Criteria*::  
A preliminary *Need → Requirement → Test* mapping exists conceptually and will be formalized
in `/docs/tests/traceability.adoc` for milestone 3.  Each requirement includes measurable
acceptance conditions:
– Interface Requirements define button-state validation, inline error messaging, and toast feedback.  
– Machine Requirements specify response-time thresholds (≤ 2 s average; ≤ 4 s peak) and uptime ≥ 99.7 %.  
– Domain Requirements link directly to test cases verifying classification and visibility logic.

*Unit Testing*::  
Manual unit tests were completed for **publishListing()**, validating data verification,
database insertion, and frontend feedback.  All manual tests passed successfully.
Additional automated unit tests for **closeListing()** and **editListing()** are scheduled for
the next milestone.  Authentication functions will be verified using Jest once the
Auth module is merged.

*Integration Testing*::  
End-to-end tests confirmed correct UI-to-database behavior: information entered in the
listing form propagates through backend validation and is stored in Supabase as expected.
Integration with Authentication (user ID linkage) will be completed in the final milestone.
The Search module’s repository functions
(`PieceRepository.getPieces()` and `filterPieces()`)
were validated indirectly through accurate data synchronization between Supabase
queries and the rendered results.

*Load and Performance Testing*::  
Preliminary manual observations show average search responses in **≈ 1 second**
and listing creation times under **0.5 seconds**—both within the defined machine-requirement limits.
Formal automated load testing using **k6** will be added to simulate concurrent usage
(150–500 users) and confirm scalability benchmarks.

*Data Validation and Security Checks*::  
Map-coordinate rendering logic now filters out invalid or non-finite latitude/longitude
values, preventing off-map markers.  
All markers are non-draggable, ensuring that location data remains immutable in the UI.
Auth and privacy testing will include Supabase RLS and access-policy verification in the
next milestone to confirm that user data is properly scoped and secured.

*Continuous Verification*::  
A GitHub Actions workflow will execute linting and unit-test jobs on pull requests to
maintain consistent quality and prevent regressions once automated tests are in place.

==== Outcomes

– Documentation, domain model, and requirements were aligned and validated through internal review cycles.  
– Listing-publication backend passed all manual unit tests, achieving < 0.5 s creation time.  
– Search and map functionalities were validated against Supabase data, loading results in ≈ 1 s on average.  
– Map markers were verified for nine sample donation centers.  
– Category and condition rating systems were refined for accuracy and uniformity.  
– Traceability structure and automated CI testing are established for completion in the final milestone.

Together, these validation and verification activities confirm that the system
concepts are sound, the current implementation behaves as specified, and measurable
criteria are in place to ensure the platform remains reliable, scalable, and aligned with
stakeholder expectations as development continues.
